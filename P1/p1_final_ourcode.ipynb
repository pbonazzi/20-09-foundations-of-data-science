{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "id1w7kwZVF3I"
   },
   "source": [
    "# Practical 1 : Implementation of Linear Regression (Ridge, Lasso)\n",
    "\n",
    "First part:\n",
    "- Implement linear regression model \n",
    "    - using least squares method\n",
    "    - implement directly using the NumPy package\n",
    "\n",
    "Second part:\n",
    "- regularization\n",
    "- polynomial basis expansion\n",
    "- cross validation\n",
    "- scikit-learn: https://scikit-learn.org/\n",
    "\n",
    "You will need to use the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "CTZv9o5i4gy3"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import _pickle as cp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b1-ZQWqTVPno"
   },
   "source": [
    "For the purpose of testing, we’ll use the winequality dataset. The dataset is available here:\n",
    "https://archive.ics.uci.edu/ml/datasets/Wine+Quality In order to make it easier to import the dataset, we’ve converted the data to the numpy array format and shuffled it so that you can start the practical directly. The dataset is available on the course website. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TzDL9RQiVaPY"
   },
   "source": [
    "The dataset has two files. We’ll focus on the white wine data, which is the larger dataset. You can load the data from the files as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NYkwbebUVO_i",
    "outputId": "7aaade1c-0f71-4245-cc6a-15455bf59490"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'winequality-white.pickle'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-1f90c38b0159>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# y is a vector of the corresponding labels of the records\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#X, y = cp.load(open('/Data/winequality-white.pickle', 'rb'))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'winequality-white.pickle'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;31m# check the size of the data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"X is a matrix with shape {}, which has {} records and {} attributes.\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'winequality-white.pickle'"
     ]
    }
   ],
   "source": [
    "# load the dataset\n",
    "# X is a matrix such that each row stores a data record \n",
    "# y is a vector of the corresponding labels of the records\n",
    "#X, y = cp.load(open('/Data/winequality-white.pickle', 'rb'))\n",
    "X, y = cp.load(open('winequality-white.pickle', 'rb'))\n",
    "# check the size of the data\n",
    "print(\"X is a matrix with shape {}, which has {} records and {} attributes.\".format(X.shape, X.shape[0], X.shape[1]))\n",
    "print(\"y is a vector with {} values, which stores the corresponding labels of the data records in X\".format(y.shape[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CGuNg0KbWN0z"
   },
   "source": [
    "In order to get consistent results, all students should use the same 80% of the data as training\n",
    "data. We’ll use the remaining as test data. To achieve this split run the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6ZqbBa8bWNYg",
    "outputId": "b661ac18-5157-46e7-c07c-d4caacbb9478"
   },
   "outputs": [],
   "source": [
    "# The function splits the dataset into the training dataset and the test dataset.\n",
    "# The parameter split_coeff is a percentage value such that\n",
    "# the first split_coeff of the dataset goes to the training dataset, \n",
    "# and the remaining data goes to the test dataset.\n",
    "def split_data(X, y, split_coeff):\n",
    "    N, _ = X.shape # get the number of records (rows)\n",
    "    train_size = int(split_coeff * N) # use the first split_coeff of the data as the training data\n",
    "    X_train = X[:train_size] # the first training_size records\n",
    "    y_train = y[:train_size]\n",
    "    X_test = X[train_size:] # the last test_size records\n",
    "    y_test = y[train_size:]\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "X_train, y_train, X_test, y_test = split_data(X, y, 0.8) # use 80% of the data as training data\n",
    "\n",
    "# check the size of the splitted dataset\n",
    "print(\"Shape of X_train:\", X_train.shape)\n",
    "print(\"Shape of y_train:\", y_train.shape)\n",
    "print(\"Shape of X_test:\", X_test.shape)\n",
    "print(\"Shape of y_test:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RL1N8mKUWYnx"
   },
   "source": [
    "We’ll not touch the test data except for reporting the errors of our learned models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q2yKNR49Wkn8"
   },
   "source": [
    "## Understanding What We’re Predicting\n",
    "\n",
    "Before we get to training a linear model on the data and using it to make predictions, let’s look\n",
    "at the spread of y values on the training set. The values are integers between 3 and 9 indicating\n",
    "the quality of the wine.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-PMpsZNSWthB"
   },
   "source": [
    "### **Task 1**\n",
    "Make a bar chart showing the distribution of y values appearing in the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 313
    },
    "id": "4L_JDK3dWrsR",
    "outputId": "066c37d4-c2b2-40b2-c35b-f16c3e9a48e7"
   },
   "outputs": [],
   "source": [
    "#@title\n",
    "# Task 1: \n",
    "# the function takes the training dataset as the input, and make the bar chart\n",
    "def plot_bar_chart_score(y_train):\n",
    "    ###################################################\n",
    "    ##### YOUR CODE STARTS HERE #######################\n",
    "    ###################################################\n",
    "    score, value = np.unique(y_train, return_counts=True)\n",
    "    \n",
    "    print(\"The scores are\" , score)\n",
    "    print(\"The values associated are\", value)\n",
    "    \n",
    "    plt.ylabel('Number of wines')\n",
    "    plt.xlabel('Score')\n",
    "    plt.bar(score, value)\n",
    "    ###################################################\n",
    "    ##### YOUR CODE ENDS HERE #########################\n",
    "    ###################################################\n",
    "\n",
    "plot_bar_chart_score(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GxjlElni2FcH"
   },
   "source": [
    "### **Task 2** \n",
    "Implement the trivial predictor, which uses the average value of y on the training set as the prediction for ever datapoint. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-V3xFYexX1lt",
    "outputId": "3bfec135-ea00-4c47-ec33-e89ee4cb37cc"
   },
   "outputs": [],
   "source": [
    "#@title\n",
    "# Task 2: implement the simplest predictor\n",
    "# The function computes the average value of y on the training label values\n",
    "def compute_average(y_train):\n",
    "    # The code below is just for compilation. \n",
    "    # You need to delete it and write your own code.\n",
    "    ###################################################\n",
    "    ##### YOUR CODE STARTS HERE #######################\n",
    "    ###################################################\n",
    "    meanY= np.mean(y_train)    \n",
    "    return meanY\n",
    "    ###################################################\n",
    "    ##### YOUR CODE ENDS HERE #########################\n",
    "    ###################################################\n",
    "\n",
    "y_train_avg = compute_average(y_train)\n",
    "print(\"Average of y on the training label values is {}\".format(y_train_avg))\n",
    "\n",
    "# The simplest predictor returns the average value.\n",
    "def simplest_predictor(X_test, y_train_avg):\n",
    "  return y_train_avg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x531Q_SxXV14"
   },
   "source": [
    "### **Task 3**\n",
    "Report the mean squared error, i.e., the average of the squared residuals, using this simplest of predictors on the training and test data. We should hope that our models beat at lease this baseline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mV8l6Ci9YlgL",
    "outputId": "198c061b-eede-4112-e256-06c255c1a110"
   },
   "outputs": [],
   "source": [
    "# We will evaluate our simplest predictor here. \n",
    "# Implement a function that can report the mean squared error \n",
    "# of a predictor on the given test data\n",
    "# Input: test dataset and predictor\n",
    "# Output: mean squared error of the predictor on the given test data\n",
    "def test_data(X_test, y_test, predictor: callable=None):\n",
    "    # Applies the predictor to each row to compute the predicted values\n",
    "    y_predicted = np.apply_along_axis(predictor, 1, X_test)\n",
    "\n",
    "    # TODO: compute the mean squared error of y_predicted\n",
    "    # The code below is just for compilation. \n",
    "    # You need to delete it and write your own code.\n",
    "    ###################################################\n",
    "    ##### YOUR CODE STARTS HERE #######################\n",
    "    ###################################################\n",
    "    errors=(y_test-y_predicted)\n",
    "    squared_errors=errors**2\n",
    "    sum_squared_errors =np.sum(squared_errors)\n",
    "    mse = 1/(y_test.size)*sum_squared_errors\n",
    "    ###################################################\n",
    "    ##### YOUR CODE ENDS HERE #########################\n",
    "    ###################################################\n",
    "    \n",
    "    return mse\n",
    "\n",
    "# use the above function test_data to evaluate the simplest predictor\n",
    "# we use the lambda function here to pass the function simplest_predictor to the evaluator.\n",
    "mse_simplest_predictor_train = test_data(X_train, y_train, lambda x: simplest_predictor(x, y_train_avg))\n",
    "mse_simplest_predictor_test = test_data(X_test, y_test, lambda x: simplest_predictor(x, y_train_avg))\n",
    "\n",
    "# Report the result\n",
    "print('Simplest Predictor')\n",
    "print('--------------------------------------------------------------------------------\\n')\n",
    "print('MSE (Training) = %.4f' % mse_simplest_predictor_train)\n",
    "print('MSE (Testing)  = %.4f' % mse_simplest_predictor_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "geiyM1Nea0az"
   },
   "source": [
    "## Linear Model Using Least Squares\n",
    "\n",
    "Let us first fit a linear regression model and then calculate the training and test error. We’ll\n",
    "actually use the closed form solution of the least squares estimate for the linear model. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cRPPA6HMbNOr"
   },
   "source": [
    "### **Task 4**\n",
    "Is it strictly necessary to standardize the data for the linear model using the least squares method? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9he5QMmfqL3_"
   },
   "source": [
    "Standardization is strictly necessary only if the model includes interaction terms a/o polynomial terms (model for curvatures, avoid the risk of producing misleading results, missing statistically significant terms, etc.). Our data set does not have these terms, therefore standardization is not required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WSEwFGp_bqAI"
   },
   "source": [
    "### **Task 5**\n",
    "Standardize the data, i.e., make the data for every feature have mean 0 and variance 1. \n",
    "\n",
    "We do the standardization using the training data, and we need to remember the means and\n",
    "the standard deviations so that they can be applied to the test data as well. Apply the\n",
    "standardization so that every feature in the training data has mean 0 and variance 1. Apply\n",
    "the same transformation to the test data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "trjwkcgybhDH",
    "outputId": "edc6db52-1b13-4b9d-d55d-5ee322073b10"
   },
   "outputs": [],
   "source": [
    "# Input: training data\n",
    "# Output: standardize training data, standard deviations and means\n",
    "def standardize_data(X):\n",
    "    # TODO: compute mean, standard deviations and the standardized data\n",
    "    # The code below is just for compilation. \n",
    "    # You need to replace it by your own code.\n",
    "    ###################################################\n",
    "    ##### YOUR CODE STARTS HERE #######################\n",
    "    ###################################################\n",
    "    mean = np.mean(X)\n",
    "    std = np.std(X)\n",
    "    X_std = (X-mean)/std \n",
    "    ###################################################\n",
    "    ##### YOUR CODE ENDS HERE #########################\n",
    "    ###################################################\n",
    "    \n",
    "    return X_std, mean, std\n",
    "\n",
    "X_train_std, X_train_mean, X_train_std_div = standardize_data(X_train)\n",
    "print(\"X_train_std:\", X_train_std.shape)\n",
    "print(\"Mean:\", X_train_mean)\n",
    "print(\"Standard deviation:\", X_train_std_div)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RjzbA5JpM759",
    "outputId": "9d99783c-d864-4f6c-a45e-0e758cb4e476"
   },
   "outputs": [],
   "source": [
    "# TODO: Standardize the test data using the mean and standard deviation you computed for the training data\n",
    "###################################################\n",
    "##### YOUR CODE STARTS HERE #######################\n",
    "###################################################\n",
    "X_test_std=(X_test-X_train_mean)/X_train_std_div\n",
    "print(X_test_std.shape)\n",
    "###################################################\n",
    "##### YOUR CODE ENDS HERE #########################\n",
    "###################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vT4_Sl42bxmD"
   },
   "source": [
    "### **Task 6**\n",
    "Implement the linear model predictor, and report the mean squared error using the linear model on the training and test data.\n",
    "\n",
    "We will do this in several steps. We need to implement the function for computing the parameters based on the training dataset. Note we need to add the bias column to the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A4JtLr6pdJV7",
    "outputId": "69e93e95-7a28-4b37-a98c-323dab30ab23"
   },
   "outputs": [],
   "source": [
    "# the function adds a column of ones to the front of the input matrix\n",
    "def expand_with_ones(X):\n",
    "    # TODO: adds a column of ones to the front of the input matrix\n",
    "    # The code below is just for compilation. \n",
    "    # You need to replace it by your own code.\n",
    "    ###################################################\n",
    "    ##### YOUR CODE STARTS HERE #######################\n",
    "    ###################################################\n",
    "    ones=np.ones((X.shape[0],1))\n",
    "    X_out = np.append(ones,X,axis=1)\n",
    "    return X_out\n",
    "    ###################################################\n",
    "    ##### YOUR CODE ENDS HERE #########################\n",
    "    ###################################################\n",
    "\n",
    "# The function computes the parameters\n",
    "def least_squares_compute_parameters(X_input, y):\n",
    "    # add the bias column to the dataset\n",
    "    X = expand_with_ones(X_input)\n",
    "\n",
    "    # TODO: compute the parameters based on the expanded X and y\n",
    "    # The code below is just for compilation. \n",
    "    # You need to replace it by your own code.\n",
    "    ###################################################\n",
    "    ##### YOUR CODE STARTS HERE #######################\n",
    "    ###################################################\n",
    "    XTX=np.dot(X.T,X)\n",
    "    T = np.linalg.multi_dot([np.linalg.inv(XTX),X.T,y])\n",
    "    return T\n",
    "    ###################################################\n",
    "    ##### YOUR CODE ENDS HERE #########################\n",
    "    ###################################################\n",
    "\n",
    "# train the linear model parameters\n",
    "w = least_squares_compute_parameters(X_train_std, y_train)\n",
    "print(\"w:\", w.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lasj_1PpeZib"
   },
   "source": [
    "We then implement the linear model predictor given the dataset and the parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lb-hNagxc3Wj"
   },
   "outputs": [],
   "source": [
    "# Implement the linear model predictor\n",
    "# Input: test data and parameters\n",
    "# Output: predicted values\n",
    "def linear_model_predictor(X, w):\n",
    "    # TODO: compute the predicted values based on the test dataset and the parameters\n",
    "    # The code below is just for compilation. \n",
    "    # You need to replace it by your own code.\n",
    "    ###################################################\n",
    "    ##### YOUR CODE STARTS HERE #######################\n",
    "    ###################################################\n",
    "    y_predicted=np.dot(X,w)\n",
    "    return y_predicted\n",
    "    ###################################################\n",
    "    ##### YOUR CODE ENDS HERE #########################\n",
    "    ###################################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cFOYpwbufz7J"
   },
   "source": [
    "We can now evaluate our linear model predictor on the test dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LuHHmn2RB55j",
    "outputId": "f1d5b75a-3268-430a-9dcc-e150d75233da"
   },
   "outputs": [],
   "source": [
    "# use the function test_data to evaluate the linear model predictor\n",
    "mse_linear_model_predictor = test_data(expand_with_ones(X_test_std), y_test, lambda x: linear_model_predictor(x, w))\n",
    "print(\"Mean squared error is {}\".format(mse_linear_model_predictor))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zqj4HKAihF7Q"
   },
   "source": [
    "## Learning Curves\n",
    "\n",
    "Let us see if the linear model is overfitting or underfitting. Since the dataset is somewhat large and there are only 11 features, our guess should be that it may either be underfitting or be about right."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XDLCsjzWhMCp"
   },
   "source": [
    "Starting with 20 datapoints, we’ll use training datasets of increasing size, in increments of 20 up to about 600 datapoints. For each case train the linear model only using the first n elements of\n",
    "the training data. Calculate the training error (on the data used) and the test error (on the full test set). Plot the training error and test error as a function of the size of the dataset used for\n",
    "training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MNf11kurCgKF"
   },
   "source": [
    "### **Task 7** \n",
    "Implement a function that evaluates the linear model over the training dataset with the input size.\n",
    "The function takes a dataset and the split coefficient as inputs, and\n",
    "1. splits the data to training and test datasets,\n",
    "2. standardizes the data,\n",
    "3. trains the linear model, and\n",
    "4. reports the mse of the linear model predictor on both training and test datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UcGRQBrEb106",
    "outputId": "194b3efb-6b9b-42fb-d28f-4911e7201523"
   },
   "outputs": [],
   "source": [
    "# Input: dataset and split coefficient\n",
    "# Output: mse of the linear model predictor on both the training and test datasets\n",
    "def train_and_test(X, y, split_coeff):\n",
    "    # TODO: implement the function \n",
    "    # The code below is just for compilation. \n",
    "    # You need to replace it by your own code.\n",
    "    ###################################################\n",
    "    ##### YOUR CODE STARTS HERE #######################\n",
    "    ###################################################\n",
    "    # Hints: use the functions you have implemented\n",
    "    X_train, y_train, X_test, y_test=split_data(X, y, split_coeff)\n",
    "    X_train_std, X_train_mean, X_train_std_div = standardize_data(X_train)\n",
    "    X_test_std = (X_test-X_train_mean)/X_train_std_div\n",
    "    \n",
    "    w = least_squares_compute_parameters(X_train_std, y_train)\n",
    "    \n",
    "    mse_train = test_data(expand_with_ones(X_train_std), y_train, lambda x: linear_model_predictor(x, w))\n",
    "    mse_test = test_data(expand_with_ones(X_test_std), y_test, lambda x: linear_model_predictor(x, w))\n",
    "    return mse_train, mse_test\n",
    "    ###################################################\n",
    "    ##### YOUR CODE ENDS HERE #########################\n",
    "    ###################################################\n",
    "\n",
    "mse_train, mse_test = train_and_test(X, y, 0.8)\n",
    "print('MSE using Linear Models')\n",
    "print('-----------------------\\n')\n",
    "print('MSE (Training) = %.4f' % mse_train)\n",
    "print('MSE (Testing)  = %.4f' % mse_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NTJw_BrzhRwi"
   },
   "source": [
    "### **Task 8**\n",
    "Report the learning curves plot. Also, explain whether you think the model is underfitting or not and how much data you need before getting the optimal test error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 316
    },
    "id": "jDsdh4T3hcIU",
    "outputId": "7ebeca8e-576d-418f-e6ec-752d7aa02df7"
   },
   "outputs": [],
   "source": [
    "mse_train_v = []\n",
    "mse_test_v = []\n",
    "K_v=[] #splitting coefficients\n",
    "training_size =[] #size of the training set\n",
    "\n",
    "TRAINING_SIZE_MAX = 601\n",
    "TRAINING_SIZE_MIN = 20\n",
    "\n",
    "# compute the errors over datasets with different sizes\n",
    "for train_size in range(TRAINING_SIZE_MIN, TRAINING_SIZE_MAX, 20):\n",
    "    # TODO: compute the training error and test error on datasets with size train_size\n",
    "    # and add them to mse_train_v and mse_test_v, respectively \n",
    "    K=train_size/X.shape[0] \n",
    "    mse_train, mse_test = train_and_test(X, y, K)\n",
    "    training_size.append(train_size)\n",
    "    K_v.append(K)\n",
    "    mse_train_v.append(mse_train)\n",
    "    mse_test_v.append(mse_test)\n",
    "\n",
    "# The below code outputs the plot of mse from different training sizes\n",
    "plt.figure(2)\n",
    "plt.plot(np.arange(TRAINING_SIZE_MIN, TRAINING_SIZE_MAX, 20), mse_train_v, 'r--', label=\"Training Error\")\n",
    "plt.plot(np.arange(TRAINING_SIZE_MIN, TRAINING_SIZE_MAX, 20), mse_test_v, 'b-', label=\"Test Error\")\n",
    "plt.xlabel('Dataset Size')\n",
    "plt.ylabel('Mean Squared Error')\n",
    "plt.show()\n",
    "\n",
    "val, idx = min((val, idx) for (idx, val) in enumerate(mse_test_v)) #find the optimal test error value and its position\n",
    "print('The optimal test error is ', val, ' and can be found using a split coefficient of ' , K_v[idx], ' equivalent to a training size of ', training_size[idx], '.' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9A9VqDTzOdfd"
   },
   "source": [
    "#### Solution Task 8:\n",
    "We did not see any underfitting since intially we started with less train dataset and when you have few train examples model tends to overfit. Similar pattern can be seen in initial datasizes where test error is far greater than train set. As we gradually increased the train size, the problem of overfitting got reduced.\n",
    "The optimal test error is  0.5674813618333658 and can be found using a split coefficient of  0.10616578195181707  equivalent to a training size of  520 . The model reaches optimality when it is fed with most of the training data. However, according to the graph line it can be seen that the model plateaus with a training set size of around 210. Slightly more than this number and the test error becomes greater than the training error, which is a clear sympthom of overfitting. If the model is used in a real life application, it would be wise to early stop the parameters calculation at 200 data points. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "djpsaTu_kK3T"
   },
   "source": [
    "## Polynomial Basis Expansion with Ridge and Lasso\n",
    "\n",
    "For this part use the following from the scikit-learn package. Read the documentation available here: http://scikit-learn.org/stable/modules/classes.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pnw2FEvqkdV_"
   },
   "source": [
    "You will need the use the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9TM0nkNbkhfM"
   },
   "outputs": [],
   "source": [
    "# You will need the following libs. \n",
    "# Fell free to import other libs. \n",
    "\n",
    "# import the preprocessing libs for standarization and basis expansion\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures \n",
    "\n",
    "# Ridge and Lasso linear model\n",
    "from sklearn.linear_model import Ridge, Lasso "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9fAfOfXCksT9"
   },
   "source": [
    "Try 5 powers of 10 for lambda from 10^-2 to 10^2 and use degree 2 basis expansion. Fit ridge and lasso using degree 2 polynomial expansion with these values of lambda. You should pick the optimal values for lambda using a validation set. Set the last 20% of the training set for the purpose of validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pCwBPuOXlRF7"
   },
   "source": [
    "### **Task 9**\n",
    "Let's implement the function for expanding the basis of the dataset. \n",
    "\n",
    "Hints: use `PolynomialFeatures`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "50azFolql1qA"
   },
   "outputs": [],
   "source": [
    "def expand_basis(X, degree):\n",
    "    # TODO: expand the basis of X for the degree\n",
    "    # The code below is just for compilation. \n",
    "    # You need to replace it by your own code.\n",
    "    ###################################################\n",
    "    ##### YOUR CODE STARTS HERE #######################\n",
    "    ###################################################\n",
    "    # Hints: use the function PolynomialFeatures\n",
    "    poly=PolynomialFeatures(degree)\n",
    "    X=poly.fit_transform(X)\n",
    "    return X\n",
    "    ###################################################\n",
    "    ##### YOUR CODE ENDS HERE #########################\n",
    "    ###################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6jwkPevimQri"
   },
   "source": [
    "### **Task 10**\n",
    "Prepare the training, test and validation data using the expanded dataset. Expand and standardize the the data. \n",
    "\n",
    "Hints: you can use `StandardScaler` and `std_scaler` to standardize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dQCq4G9YmW7w",
    "outputId": "b3e9af4e-0b95-4d8d-8172-a2e7ce9451de"
   },
   "outputs": [],
   "source": [
    "# TODO: the training, test and validation data using the expanded dataset.\n",
    "# The code below is just for compilation. \n",
    "# You need to replace it by your own code.\n",
    "def prepare_data(X, y, degree):\n",
    "    ###################################################\n",
    "    ##### YOUR CODE STARTS HERE #######################\n",
    "    ###################################################\n",
    "    # Hints: follow the steps    \n",
    "    # You need to parpare four datasets:\n",
    "    # 1. training data -- X_train, y_train\n",
    "    # 2. test data -- X_test, y_test\n",
    "    # 3. validation data -- X_train_v, y_train_v\n",
    "    # 4. training data (cross validation) -- X_train_n, y_train_n\n",
    "    \n",
    "    # You need expand the basis of the data, and do standardization\n",
    "    X_expand=expand_basis(X,degree)\n",
    "    X_train,y_train,X_test,y_test=split_data(X_expand,y,0.8)\n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    #training_Data\n",
    "    X_train=scaler.fit_transform(X_train)\n",
    "    \n",
    "    # test data\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    # further split the training data to training and validation data\n",
    "    # training data\n",
    "    X_train_n, y_train_n, X_train_v, y_train_v= split_data(X_train,y_train,0.8) \n",
    "\n",
    "    # validation data\n",
    "    \n",
    "\n",
    "    return X_train, y_train, X_train_n, y_train_n, X_train_v, y_train_v, X_test, y_test\n",
    "    ###################################################\n",
    "    ##### YOUR CODE ENDS HERE #########################\n",
    "    ###################################################\n",
    "\n",
    "X_train, y_train, X_train_n, y_train_n, X_train_v, y_train_v, X_test, y_test = prepare_data(X, y, 2)# here we expand the dataset with degree 2\n",
    "print(X_train.shape,X_train_v.shape,X_train_n.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i3BxxtM3nghU"
   },
   "source": [
    "### **Task 11**\n",
    "We have prepared the training data and the validation data. We can now choose the hyper parameter lambda for Ridge and Lasso using the validation data. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 559
    },
    "id": "SvXcAGW1oHq1",
    "outputId": "4d2465f1-a56d-4255-c7ff-9a865ea76dde"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "# The function takes the training and validation data as inputs, and \n",
    "# returns the lambda value that has the minimal mse\n",
    "# We use is_ridge to indicate the model we consider. \n",
    "# is_ridge = True indicates Ridge while is_ridge = False indicates Lasso\n",
    "def choose_hyper_param(X_train_n, y_train_n, X_train_v, y_train_v, is_ridge: bool):\n",
    "    mse_arr = []\n",
    "    lam_arr = []\n",
    "\n",
    "    # Try lambda values from 10^-2 to 10^2. \n",
    "    # Record the mse and the lambda values in mse_arr and lam_arr\n",
    "    # The code below is just for compilation. \n",
    "    # You need to replace it by your own code.\n",
    "    ###################################################\n",
    "    ##### YOUR CODE STARTS HERE #######################\n",
    "    ###################################################\n",
    "    for pow_lam in range(-2, 3):\n",
    "        lam = 10 ** pow_lam\n",
    "        if is_ridge:\n",
    "            clf=Ridge(lam)\n",
    "        else:\n",
    "            clf=Lasso(lam)\n",
    "        clf.fit(X_train_n,y_train_n)\n",
    "\n",
    "        y_predict= clf.predict(X_train_v)\n",
    "        mse_val=mean_squared_error(y_train_v,y_predict)\n",
    "        mse_arr.append(mse_val) # add the mse when using the hyperparameter lam\n",
    "        lam_arr.append(lam)\n",
    "    ###################################################\n",
    "    ##### YOUR CODE ENDS HERE #########################\n",
    "    ###################################################\n",
    "\n",
    "    # get the index of the lambda value that has the minimal use\n",
    "    lambda_idx_min = np.argmin(np.array(mse_arr))\n",
    "\n",
    "    # plot of the lambda values and their mse\n",
    "    plt.figure()\n",
    "    plt.semilogx(lam_arr, mse_arr)\n",
    "\n",
    "    # return the best lambda value\n",
    "    return lam_arr[lambda_idx_min]\n",
    "\n",
    "# call the function to choose the lambda for Ridge and Lasso\n",
    "lam_ridge = choose_hyper_param(X_train_n, y_train_n, X_train_v, y_train_v, True)\n",
    "lam_lasso = choose_hyper_param(X_train_n, y_train_n, X_train_v, y_train_v, False)\n",
    "\n",
    "print(\"Ridge lambda:\", lam_ridge)\n",
    "print(\"Lasso lambda:\", lam_lasso)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FAuX0uU5k9qD"
   },
   "source": [
    "### **Task 12**:\n",
    "Once you’ve obtained the optimal values for lambda for Ridge and Lasso, train these models using these hyperparameters on the full training data. Then report\n",
    "the training and test error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VmwHESkg77zK",
    "outputId": "63f4e7a4-e31a-4de8-f617-a9f1a91503cd"
   },
   "outputs": [],
   "source": [
    "# TODO: train the Ridge and Lasso models using their best parameters, and\n",
    "#       report their mse\n",
    "###################################################\n",
    "##### YOUR CODE STARTS HERE #######################\n",
    "###################################################\n",
    "# Hints: train these models on the full training data\n",
    "clf_ridge=Ridge(lam_ridge)\n",
    "clf_ridge.fit(X_train, y_train)\n",
    "y_train_ridge_predict=clf_ridge.predict(X_train)\n",
    "y_test_ridge_predict=clf_ridge.predict(X_test)\n",
    "\n",
    "clf_lasso=Lasso(lam_lasso,max_iter=10000)\n",
    "clf_lasso.fit(X_train, y_train)\n",
    "y_train_lasso_predict=clf_lasso.predict(X_train)\n",
    "y_test_lasso_predict=clf_lasso.predict(X_test)\n",
    "\n",
    "mse_ridge_train = mean_squared_error(y_train,y_train_ridge_predict)\n",
    "mse_ridge_test = mean_squared_error(y_test,y_test_ridge_predict)\n",
    "mse_lasso_train = mean_squared_error(y_train,y_train_lasso_predict)\n",
    "mse_lasso_test = mean_squared_error(y_test,y_test_lasso_predict)\n",
    "###################################################\n",
    "##### YOUR CODE ENDS HERE #########################\n",
    "###################################################\n",
    "\n",
    "# Report the result\n",
    "print('For Ridge Regression with using degree %d polynomial expansion and lambda = %.4f' % (2, lam_ridge))\n",
    "print('--------------------------------------------------------------------------------\\n')\n",
    "print('MSE (Training) = %.4f' % mse_ridge_train)\n",
    "print('MSE (Testing)  = %.4f' % mse_ridge_test)\n",
    "\n",
    "print('\\n\\nFor Lasso with using degree %d polynomial expansion and lambda = %.4f' % (2, lam_lasso))\n",
    "print('---------------------------------------------------------------------\\n')\n",
    "print('MSE (Training) = %.4f' % mse_lasso_train)\n",
    "print('MSE (Testing)  = %.4f' % mse_lasso_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Os9tKKLd8gMU"
   },
   "source": [
    "## Larger Degrees\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sfqRAlv1PBXi"
   },
   "source": [
    "### **Task 13**\n",
    "Try using higher degree basis expansion. You may want to use k-fold cross validation to determine\n",
    "the values of hyperparameters rather than just keeping a validation set. \n",
    "\n",
    "Hints: Use `KFold` to do this automatically. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kpwY7UtQ8l-0",
    "outputId": "01b437ad-ecd1-4286-f7c2-34a7cc0aca45"
   },
   "outputs": [],
   "source": [
    "# KFold\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# TODO: Try using higher degree basis expansion. Find the degree that gives the minimal mse. \n",
    "###################################################\n",
    "##### YOUR CODE STARTS HERE #######################\n",
    "###################################################\n",
    "# Hints: use KFold\n",
    "def search_hyperparameter_using_kfold(X_train, y_train, kfold_splits, is_ridge:bool):\n",
    "    mse_arr_lambda = []\n",
    "    lam_arr = []\n",
    "\n",
    "    for pow_lam in range(-2, 3):\n",
    "        lam = 10 ** pow_lam\n",
    "        kf = KFold(n_splits=kfold_splits)\n",
    "        mse_kfold_arr=[]\n",
    "        for train_index, test_index in kf.split(X_train):\n",
    "            X_train_n, X_train_v = X_train[train_index], X_train[test_index]\n",
    "            y_train_n, y_train_v = y_train[train_index], y_train[test_index]\n",
    "            \n",
    "            if is_ridge:\n",
    "                clf=Ridge(lam)\n",
    "            else:\n",
    "                clf=Lasso(lam,iter=10000)\n",
    "                \n",
    "            clf.fit(X_train_n,y_train_n)\n",
    "            y_predict_v= clf.predict(X_train_v)\n",
    "            mse_fold=mean_squared_error(y_train_v,y_predict_v)\n",
    "            mse_kfold_arr.append(mse_fold)\n",
    "            \n",
    "        #store average mse for each lambda    \n",
    "        mse_val_lambda=np.mean(mse_kfold_arr)\n",
    "        mse_arr_lambda.append(mse_val_lambda)\n",
    "        lam_arr.append(lam)\n",
    "        \n",
    "    #pick minimum mse and lambda    \n",
    "    lambda_idx_min = np.argmin(np.array(mse_arr_lambda))\n",
    "    \n",
    "    #store degree mse by selecting best lambda\n",
    "    best_lambda=lam_arr[lambda_idx_min]\n",
    "       \n",
    "    return best_lambda\n",
    "\n",
    "#higher degree basis expansion\n",
    "best_lamda_arr=[]\n",
    "deg_arr=[]\n",
    "mse_deg_arr=[]\n",
    "for deg in range(2,6):\n",
    "    \n",
    "    #expand\n",
    "    X_expanded=expand_basis(X,deg)\n",
    "    #split only for test\n",
    "    X_train,y_train,X_test,y_test=split_data(X_expanded,y,0.8)\n",
    "    \n",
    "    #standardise\n",
    "    scaler = StandardScaler()\n",
    "    X_train=scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    \n",
    "    #hyperparameter search\n",
    "    kfold_splits=5\n",
    "    best_lambda=search_hyperparameter_using_kfold(X_train,y_train,kfold_splits,True)\n",
    "    best_lamda_arr.append(best_lambda)\n",
    "    \n",
    "    #train clasifier on train set with best lambda\n",
    "    clf=Ridge(best_lambda)\n",
    "    clf.fit(X_train,y_train)\n",
    "    y_predict_test= clf.predict(X_test)\n",
    "    mse_deg=mean_squared_error(y_test,y_predict_test)\n",
    "    \n",
    "    mse_deg_arr.append(mse_deg)\n",
    "    deg_arr.append(deg)\n",
    "\n",
    "# Degree with minimum mse\n",
    "degree_idx_min=np.argmin(np.array(mse_deg_arr))    \n",
    "print(\"%d is the degree with minimal mse=%.4f\"%(deg_arr[degree_idx_min],mse_deg_arr[degree_idx_min]))   \n",
    "\n",
    "for i in range(4):\n",
    "    print('\\nFor Ridge with using degree %d polynomial expansion and lambda = %.4f, MSE = %.4f '% (deg_arr[i], best_lamda_arr[i],mse_deg_arr[i]))\n",
    "###################################################\n",
    "##### YOUR CODE ENDS HERE #########################\n",
    "###################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JYXi4BSAUSWJ"
   },
   "source": [
    "### Project Short Report\n",
    "\n",
    "Task 1 : The skeleton code was slightly modified because X_train was redundant.The method first finds the number of times where each unique score appears, than it returns a bar chart.\n",
    "\n",
    "Task 2 : The function simply take y_train as an input and it computes the mean with the numpy built-in method.\n",
    "\n",
    "Task 3 : The MSE was computed using the formula provided in the lecture, the process was performed step by step.\n",
    "\n",
    "Task 4 : See answer in the given text box.\n",
    "\n",
    "Task 5 : The method to standardize the data make use of the numpy methods \"mean\" and \"std\". The standardization of the test dataset use the mean and the standard deviation of the train dataset.\n",
    "\n",
    "Task 6 : First, a column of one has been addeded in order to make use of the matrix formula for computing the linear model parameters. Then, the numpy method linalg.multi_dot was used to compute the dot product of two or more arrays in a single function call, while automatically selecting the fastest evaluation order. Finally, the target variable was calculate via a simple numpy dot product.\n",
    "\n",
    "Task 7 : For this exercise, we make use of the functions previously computed. In sequence, we tell the program to split the data set, to standardize the training and testing X set, to compute the parameters and to return the MSE after having computed the predictions.\n",
    "\n",
    "Task 8 : See answer in the given text box.\n",
    "\n",
    "Task 9 : The function uses the scikit-learn method to expand each feature column of the X array with its polynomial equivalent for all the degrees used in the input.\n",
    "\n",
    "Task 10 : For this exercise, we followed the steps provided in the skeleton code and made full use of the scikit-learn built in method.\n",
    "\n",
    "Task 11 : The function selects the hyper parameter lambda that minimizes the MSE. This function was later tested to select hyperparameters for the NBC classifier in pratical 2.\n",
    "\n",
    "Task 12 : The lambdas derived in the previous task were applied here. To remove a warning in the Lasso model, we set max_iter=10000. \n",
    "\n",
    "Task 13 : First, a function was compute to search the best hyperparameter using the K-Fold method. This function was applied after basis expansion and standardization of the independant variables in order to find the degree (for basis expansion) that minimize the MSE. Using a range between 2 and 6, we discover that the best degree is 3. We report that we have encountered the well known fact that basis expansion helps reduce the MSE of a model, but it has a negative effect when it is used too extensively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "p1_final_ourcode.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
