{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "id1w7kwZVF3I"
   },
   "source": [
    "# Practical 1 : Implementation of Linear Regression (Ridge, Lasso)\n",
    "\n",
    "First part:\n",
    "- Implement linear regression model \n",
    "    - using least squares method\n",
    "    - implement directly using the NumPy package\n",
    "\n",
    "Second part:\n",
    "- regularization\n",
    "- polynomial basis expansion\n",
    "- cross validation\n",
    "- scikit-learn: https://scikit-learn.org/\n",
    "\n",
    "You will need to use the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CTZv9o5i4gy3"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import _pickle as cp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b1-ZQWqTVPno"
   },
   "source": [
    "For the purpose of testing, we’ll use the winequality dataset. The dataset is available here:\n",
    "https://archive.ics.uci.edu/ml/datasets/Wine+Quality In order to make it easier to import the dataset, we’ve converted the data to the numpy array format and shuffled it so that you can start the practical directly. The dataset is available on the course website. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TzDL9RQiVaPY"
   },
   "source": [
    "The dataset has two files. We’ll focus on the white wine data, which is the larger dataset. You can load the data from the files as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1423,
     "status": "ok",
     "timestamp": 1596436129238,
     "user": {
      "displayName": "Haozhe Zhang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhewCb1FImnjURCLugyfalL8wVXJomnuoEHUckN=s64",
      "userId": "15943369882491692800"
     },
     "user_tz": -480
    },
    "id": "NYkwbebUVO_i",
    "outputId": "80ed8916-85c3-4564-cda8-d8a8f36aaa1d"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'winequality-white.pickle'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-8ee2b09bb0b0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# X is a matrix such that each row stores a data record\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# y is a vector of the corresponding labels of the records\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'winequality-white.pickle'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# check the size of the data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'winequality-white.pickle'"
     ]
    }
   ],
   "source": [
    "# load the dataset\n",
    "# X is a matrix such that each row stores a data record \n",
    "# y is a vector of the corresponding labels of the records\n",
    "X, y = cp.load(open('winequality-white.pickle', 'rb'))\n",
    "\n",
    "# check the size of the data\n",
    "print(\"X is a matrix with shape {}, which has {} records and {} attributes.\".format(X.shape, X.shape[0], X.shape[1]))\n",
    "print(\"y is a vector with {} values, which stores the corresponding labels of the data records in X\".format(y.shape[0]))\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CGuNg0KbWN0z"
   },
   "source": [
    "In order to get consistent results, all students should use the same 80% of the data as training\n",
    "data. We’ll use the remaining as test data. To achieve this split run the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1416,
     "status": "ok",
     "timestamp": 1596436129239,
     "user": {
      "displayName": "Haozhe Zhang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhewCb1FImnjURCLugyfalL8wVXJomnuoEHUckN=s64",
      "userId": "15943369882491692800"
     },
     "user_tz": -480
    },
    "id": "6ZqbBa8bWNYg",
    "outputId": "da274c4e-c3ed-4ac0-8442-27befcf26f4c"
   },
   "outputs": [],
   "source": [
    "# The function splits the dataset into the training dataset and the test dataset.\n",
    "# The parameter split_coeff is a percentage value such that\n",
    "# the first split_coeff of the dataset goes to the training dataset, \n",
    "# and the remaining data goes to the test dataset.\n",
    "def split_data(X, y, split_coeff):\n",
    "    N, _ = X.shape # get the number of records (rows)\n",
    "    train_size = int(split_coeff * N) # use the first split_coeff of the data as the training data\n",
    "    X_train = X[:train_size] # the first training_size records\n",
    "    y_train = y[:train_size]\n",
    "    X_test = X[train_size:] # the last test_size records\n",
    "    y_test = y[train_size:]\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "X_train, y_train, X_test, y_test = split_data(X, y, 0.8) # use 80% of the data as training data\n",
    "\n",
    "# check the size of the splitted dataset\n",
    "print(\"Shape of X_train:\", X_train.shape)\n",
    "print(\"Shape of y_train:\", y_train.shape)\n",
    "print(\"Shape of X_test:\", X_test.shape)\n",
    "print(\"Shape of y_test:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RL1N8mKUWYnx"
   },
   "source": [
    "We’ll not touch the test data except for reporting the errors of our learned models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q2yKNR49Wkn8"
   },
   "source": [
    "## Understanding What We’re Predicting\n",
    "\n",
    "Before we get to training a linear model on the data and using it to make predictions, let’s look\n",
    "at the spread of y values on the training set. The values are integers between 3 and 9 indicating\n",
    "the quality of the wine.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-PMpsZNSWthB"
   },
   "source": [
    "### **Task 1**\n",
    "Make a bar chart showing the distribution of y values appearing in the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1409,
     "status": "ok",
     "timestamp": 1596436129240,
     "user": {
      "displayName": "Haozhe Zhang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhewCb1FImnjURCLugyfalL8wVXJomnuoEHUckN=s64",
      "userId": "15943369882491692800"
     },
     "user_tz": -480
    },
    "id": "4L_JDK3dWrsR",
    "outputId": "71b22bf6-77ce-4bd6-d5b1-61f633923144"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-6da7f7663400>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;31m###################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mplot_bar_chart_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "#@title\n",
    "# Task 1: \n",
    "# the function takes the training dataset as the input, and make the bar chart\n",
    "def plot_bar_chart_score(X_train, y_train):\n",
    "    ###################################################\n",
    "    ##### YOUR CODE STARTS HERE #######################\n",
    "    ###################################################\n",
    "    score, value = np.unique(y_train, return_counts=True)\n",
    "    dict(zip(score, value))\n",
    "    \n",
    "    print(\"The scores are\" , score)\n",
    "    print(\"The values associated are\", value)\n",
    "    \n",
    "    plt.ylabel('Number of wines')\n",
    "    plt.xlabel('Score')\n",
    "    plt.bar(score, value)\n",
    "    ###################################################\n",
    "    ##### YOUR CODE ENDS HERE #########################\n",
    "    ###################################################\n",
    "\n",
    "plot_bar_chart_score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GxjlElni2FcH"
   },
   "source": [
    "### **Task 2** \n",
    "Implement the trivial predictor, which uses the average value of y on the training set as the prediction for ever datapoint. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1402,
     "status": "ok",
     "timestamp": 1596436129240,
     "user": {
      "displayName": "Haozhe Zhang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhewCb1FImnjURCLugyfalL8wVXJomnuoEHUckN=s64",
      "userId": "15943369882491692800"
     },
     "user_tz": -480
    },
    "id": "-V3xFYexX1lt",
    "outputId": "5e57738e-87d5-408c-f1bf-9df66a175f35"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-de085aff95aa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m###################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0my_train_avg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_average\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Average of y on the training label values is {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train_avg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'y_train' is not defined"
     ]
    }
   ],
   "source": [
    "#@title\n",
    "# Task 2: implement the simplest predictor\n",
    "# The function computes the average value of y on the training label values\n",
    "def compute_average(y_train):\n",
    "    ###################################################\n",
    "    ##### YOUR CODE STARTS HERE #######################\n",
    "    ###################################################\n",
    "    # Hint: return the mean of y\n",
    "    mean_Y= np.mean(y_train)    \n",
    "    return mean_Y\n",
    "    ###################################################\n",
    "    ##### YOUR CODE ENDS HERE #########################\n",
    "    ###################################################\n",
    "\n",
    "y_train_avg = compute_average(y_train)\n",
    "print(\"Average of y on the training label values is {}\".format(y_train_avg))\n",
    "\n",
    "# The simplest predictor returns the average value.\n",
    "def simplest_predictor(X_test, y_train_avg):\n",
    "    return y_train_avg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x531Q_SxXV14"
   },
   "source": [
    "### **Task 3**\n",
    "Report the mean squared error, i.e., the average of the squared residuals, using this simplest of predictors on the training and test data. We should hope that our models beat at lease this baseline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1394,
     "status": "ok",
     "timestamp": 1596436129240,
     "user": {
      "displayName": "Haozhe Zhang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhewCb1FImnjURCLugyfalL8wVXJomnuoEHUckN=s64",
      "userId": "15943369882491692800"
     },
     "user_tz": -480
    },
    "id": "mV8l6Ci9YlgL",
    "outputId": "f57858dc-d0fc-40fe-dbf7-c652d2f8fddb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simplest Predictor\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "MSE (Training) = 0.7768\n",
      "MSE (Testing)  = 0.8139\n"
     ]
    }
   ],
   "source": [
    "# We will evaluate our simplest predictor here. \n",
    "# Implement a function that can report the mean squared error \n",
    "# of a predictor on the given test data\n",
    "# Input: test dataset and predictor\n",
    "# Output: mean squared error of the predictor on the given test data\n",
    "def test_data(X_test, y_test, predictor: callable=None):\n",
    "    # Applies the predictor to each row to compute the predicted values\n",
    "    y_predicted = np.apply_along_axis(predictor, 1, X_test)\n",
    "\n",
    "    # TODO: compute the mean squared error of y_predicted\n",
    "    # The code below is just for compilation. \n",
    "    # You need to delete it and write your own code.\n",
    "    ###################################################\n",
    "    ##### YOUR CODE STARTS HERE #######################\n",
    "    ###################################################\n",
    "    \n",
    "    errors=(y_test-y_predicted)\n",
    "    squared_errors=errors**2\n",
    "    sum_squared_errors =np.sum(squared_errors)\n",
    "    mse = 1/(y_test.size)*sum_squared_errors\n",
    "    ###################################################\n",
    "    ##### YOUR CODE ENDS HERE #########################\n",
    "    ###################################################\n",
    "    \n",
    "    return mse\n",
    "\n",
    "# use the above function test_data to evaluate the simplest predictor\n",
    "# we use the lambda function here to pass the function simplest_predictor to the evaluator.\n",
    "mse_simplest_predictor_train = test_data(X_train, y_train, lambda x: simplest_predictor(x, y_train_avg))\n",
    "mse_simplest_predictor_test = test_data(X_test, y_test, lambda x: simplest_predictor(x, y_train_avg))\n",
    "\n",
    "# Report the result\n",
    "print('Simplest Predictor')\n",
    "print('--------------------------------------------------------------------------------\\n')\n",
    "print('MSE (Training) = %.4f' % mse_simplest_predictor_train)\n",
    "print('MSE (Testing)  = %.4f' % mse_simplest_predictor_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "geiyM1Nea0az"
   },
   "source": [
    "## Linear Model Using Least Squares\n",
    "\n",
    "Let us first fit a linear regression model and then calculate the training and test error. We’ll\n",
    "actually use the closed form solution of the least squares estimate for the linear model. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cRPPA6HMbNOr"
   },
   "source": [
    "### **Task 4**\n",
    "Is it strictly necessary to standardize the data for the linear model using the least squares method? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9he5QMmfqL3_"
   },
   "source": [
    "(Add the answer to Task 4 here in text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WSEwFGp_bqAI"
   },
   "source": [
    "### **Task 5**\n",
    "Standardize the data, i.e., make the data for every feature have mean 0 and variance 1. \n",
    "\n",
    "We do the standardization using the training data, and we need to remember the means and\n",
    "the standard deviations so that they can be applied to the test data as well. Apply the\n",
    "standardization so that every feature in the training data has mean 0 and variance 1. Apply\n",
    "the same transformation to the test data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1387,
     "status": "ok",
     "timestamp": 1596436129241,
     "user": {
      "displayName": "Haozhe Zhang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhewCb1FImnjURCLugyfalL8wVXJomnuoEHUckN=s64",
      "userId": "15943369882491692800"
     },
     "user_tz": -480
    },
    "id": "trjwkcgybhDH",
    "outputId": "d87a4635-354f-47e2-947a-e843f027e4cb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_std: (3918, 11)\n",
      "Mean: 18.45094144913917\n",
      "Standard deviation: 41.579203628771\n"
     ]
    }
   ],
   "source": [
    "# Input: training data\n",
    "# Output: standardize training data, standard deviations and means\n",
    "def standardize_data(X):\n",
    "    # TODO: compute mean, standard deviations and the standardized data\n",
    "    # The code below is just for compilation. \n",
    "    # You need to replace it by your own code.\n",
    "    ###################################################\n",
    "    ##### YOUR CODE STARTS HERE #######################\n",
    "    ###################################################\n",
    "    mean = np.mean(X)\n",
    "    std = np.std(X)\n",
    "    X_std = (X-mean)/std \n",
    "   \n",
    "    ###################################################\n",
    "    ##### YOUR CODE ENDS HERE #########################\n",
    "    ###################################################\n",
    "    \n",
    "    return X_std, mean, std\n",
    "\n",
    "X_train_std, X_train_mean, X_train_std_div = standardize_data(X_train)\n",
    "print(\"X_train_std:\", X_train_std.shape)\n",
    "print(\"Mean:\", X_train_mean)\n",
    "print(\"Standard deviation:\", X_train_std_div)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1380,
     "status": "ok",
     "timestamp": 1596436129242,
     "user": {
      "displayName": "Haozhe Zhang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhewCb1FImnjURCLugyfalL8wVXJomnuoEHUckN=s64",
      "userId": "15943369882491692800"
     },
     "user_tz": -480
    },
    "id": "RjzbA5JpM759",
    "outputId": "ff594788-2fdd-419c-98fa-beac6a53cfc9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(980, 11)\n"
     ]
    }
   ],
   "source": [
    "# TODO: Standardize the test data using the mean and standrad deviation you computed for the training data\n",
    "###################################################\n",
    "##### YOUR CODE STARTS HERE #######################\n",
    "###################################################\n",
    "\n",
    "X_test_std = (X_test-X_train_mean)/X_train_std_div \n",
    "print(X_test_std.shape)\n",
    "###################################################\n",
    "##### YOUR CODE ENDS HERE #########################\n",
    "###################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vT4_Sl42bxmD"
   },
   "source": [
    "### **Task 6**\n",
    "Implement the linear model predictor, and report the mean squared error using the linear model on the training and test data.\n",
    "\n",
    "We will do this in several steps. We need to implement the function for computing the parameters based on the training dataset. Note we need to add the bias column to the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1374,
     "status": "ok",
     "timestamp": 1596436129242,
     "user": {
      "displayName": "Haozhe Zhang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhewCb1FImnjURCLugyfalL8wVXJomnuoEHUckN=s64",
      "userId": "15943369882491692800"
     },
     "user_tz": -480
    },
    "id": "A4JtLr6pdJV7",
    "outputId": "dfd57312-284f-4ce9-820b-4fdbdfbec8c4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w: (12,)\n"
     ]
    }
   ],
   "source": [
    "# the function adds a column of ones to the front of the input matrix\n",
    "def expand_with_ones(X):\n",
    "    # TODO: adds a column of ones to the front of the input matrix\n",
    "    # The code below is just for compilation. \n",
    "    # You need to replace it by your own code.\n",
    "    ###################################################\n",
    "    ##### YOUR CODE STARTS HERE #######################\n",
    "    ###################################################\n",
    "    np_ones = np.ones((X.shape[0],1))\n",
    "    X_out = np.hstack((X,np_ones))\n",
    "    return X_out\n",
    "    ###################################################\n",
    "    ##### YOUR CODE ENDS HERE #########################\n",
    "    ###################################################\n",
    "\n",
    "# The function computes the parameters\n",
    "def least_squares_compute_parameters(X_input, y):\n",
    "    # add the bias column to the dataset\n",
    "    X = expand_with_ones(X_input)\n",
    "\n",
    "    # TODO: compute the parameters based on the expanded X and y\n",
    "    cov = np.linalg.inv((X.T).dot(X))\n",
    "    T = np.dot(cov,((X.T).dot(y)))\n",
    "    ###################################################\n",
    "    ##### YOUR CODE STARTS HERE #######################\n",
    "    ###################################################\n",
    "    \n",
    "    return T\n",
    "    ###################################################\n",
    "    ##### YOUR CODE ENDS HERE #########################\n",
    "    ###################################################\n",
    "\n",
    "# train the linear model parameters\n",
    "w = least_squares_compute_parameters(X_train_std, y_train) \n",
    "print(\"w:\", w.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lasj_1PpeZib"
   },
   "source": [
    "We then implement the linear model predictor given the dataset and the parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Lb-hNagxc3Wj"
   },
   "outputs": [],
   "source": [
    "# Implement the linear model predictor\n",
    "# Input: test data and parameters\n",
    "# Output: predicted values\n",
    "def linear_model_predictor(X, w):\n",
    "    # TODO: compute the predicted values based on the test dataset and the parameters\n",
    "    # The code below is just for compilation. \n",
    "    # You need to replace it by your own code.\n",
    "    ###################################################\n",
    "    ##### YOUR CODE STARTS HERE #######################\n",
    "    ###################################################\n",
    "    y_predicted = np.dot(X,w)\n",
    "    return y_predicted\n",
    "    ###################################################\n",
    "    ##### YOUR CODE ENDS HERE #########################\n",
    "    ###################################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cFOYpwbufz7J"
   },
   "source": [
    "We can now evaluate our linear model predictor on the test dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1363,
     "status": "ok",
     "timestamp": 1596436129243,
     "user": {
      "displayName": "Haozhe Zhang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhewCb1FImnjURCLugyfalL8wVXJomnuoEHUckN=s64",
      "userId": "15943369882491692800"
     },
     "user_tz": -480
    },
    "id": "LuHHmn2RB55j",
    "outputId": "b6cb4556-2618-419a-a082-214f2e6ecb5e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error is 0.5607291967819803\n"
     ]
    }
   ],
   "source": [
    "# use the function test_data to evaluate the linear model predictor\n",
    "mse_linear_model_predictor = test_data(expand_with_ones(X_test_std), y_test, lambda x: linear_model_predictor(x, w))\n",
    "print(f\"Mean squared error is {mse_linear_model_predictor}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zqj4HKAihF7Q"
   },
   "source": [
    "## Learning Curves\n",
    "\n",
    "Let us see if the linear model is overfitting or underfitting. Since the dataset is somewhat large and there are only 11 features, our guess should be that it may either be underfitting or be about right."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XDLCsjzWhMCp"
   },
   "source": [
    "Starting with 20 datapoints, we’ll use training datasets of increasing size, in increments of 20 up to about 600 datapoints. For each case train the linear model only using the first n elements of\n",
    "the training data. Calculate the training error (on the data used) and the test error (on the full test set). Plot the training error and test error as a function of the size of the dataset used for\n",
    "training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MNf11kurCgKF"
   },
   "source": [
    "### **Task 7** \n",
    "Implement a function that evaluates the linear model over the training dataset with the input size.\n",
    "The function takes a dataset and the split coefficient as inputs, and\n",
    "1. splits the data to training and test datasets,\n",
    "2. standardizes the data,\n",
    "3. trains the linear model, and\n",
    "4. reports the mse of the linear model predictor on both training and test datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1355,
     "status": "ok",
     "timestamp": 1596436129244,
     "user": {
      "displayName": "Haozhe Zhang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhewCb1FImnjURCLugyfalL8wVXJomnuoEHUckN=s64",
      "userId": "15943369882491692800"
     },
     "user_tz": -480
    },
    "id": "UcGRQBrEb106",
    "outputId": "179c5ec0-ee87-4c4b-a02b-d97d55862e1f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE using Linear Models\n",
      "-----------------------\n",
      "\n",
      "MSE (Training) = 0.5535\n",
      "MSE (Testing)  = 0.5682\n"
     ]
    }
   ],
   "source": [
    "# Input: dataset and split coefficient\n",
    "# Output: mse of the linear model predictor on both the training and test datasets\n",
    "def train_and_test(X, y, split_coeff):\n",
    "    # TODO: implement the function \n",
    "    # The code below is just for compilation. \n",
    "    # You need to replace it by your own code.\n",
    "    ###################################################\n",
    "    ##### YOUR CODE STARTS HERE #######################\n",
    "    ###################################################\n",
    "    X_train, y_train, X_test, y_test = split_data(X, y, split_coeff)\n",
    "\n",
    "#Standardizing the data:\n",
    "    X_train_std, X_train_mean, X_train_std_div = standardize_data(X_train)\n",
    "    X_test_std = (X_test-X_train_mean)/X_train_std_div \n",
    "    \n",
    "    w = least_squares_compute_parameters(X_train_std, y_train)\n",
    "    mse_test = test_data(expand_with_ones(X_test_std), y_test, lambda x: linear_model_predictor(x, w))\n",
    "    mse_train = test_data(expand_with_ones(X_train_std), y_train, lambda x: linear_model_predictor(x, w))\n",
    "\n",
    "    return mse_train, mse_test\n",
    "    ###################################################\n",
    "    ##### YOUR CODE ENDS HERE #########################\n",
    "    ###################################################\n",
    "\n",
    "mse_train, mse_test = train_and_test(X, y, 0.1)\n",
    "print('MSE using Linear Models')\n",
    "print('-----------------------\\n')\n",
    "print('MSE (Training) = %.4f' % mse_train)\n",
    "print('MSE (Testing)  = %.4f' % mse_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NTJw_BrzhRwi"
   },
   "source": [
    "### **Task 8**\n",
    "Report the learning curves plot. Also, explain whether you think the model is underfitting or not and how much data you need before getting the optimal test error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1747,
     "status": "ok",
     "timestamp": 1596436129644,
     "user": {
      "displayName": "Haozhe Zhang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhewCb1FImnjURCLugyfalL8wVXJomnuoEHUckN=s64",
      "userId": "15943369882491692800"
     },
     "user_tz": -480
    },
    "id": "jDsdh4T3hcIU",
    "outputId": "621c4890-1c55-4e9b-f28f-33d60907d8b9"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4898, 11)\n"
     ]
    }
   ],
   "source": [
    "mse_train_v = []\n",
    "mse_test_v = []\n",
    "\n",
    "TRAINING_SIZE_MAX = 601\n",
    "TRAINING_SIZE_MIN = 20\n",
    "\n",
    "# compute the errors over datasets with different sizes\n",
    "for train_size in range(TRAINING_SIZE_MIN, TRAINING_SIZE_MAX, 20):\n",
    "    # TODO: compute the training error and test error on datasets with size train_size\n",
    "    # and add them to mse_train_v and mse_test_v, respectively\n",
    "    # The code below is just for compilation. \n",
    "    # You need to replace it by your own code.\n",
    "    ###################################################\n",
    "    ##### YOUR CODE STARTS HERE #######################\n",
    "    ###################################################\n",
    "    coeff_split = train_size/X.shape[0]\n",
    "    mse_train, mse_test = train_and_test(X, y, coeff_split)\n",
    "#     print(mse_train, mse_test)\n",
    "    ###################################################\n",
    "    ##### YOUR CODE ENDS HERE #########################\n",
    "    ###################################################\n",
    "    mse_train_v.append(mse_train)\n",
    "    mse_test_v.append(mse_test)\n",
    "\n",
    "# The below code outputs the plot of mse from different training sizes\n",
    "plt.figure(2)\n",
    "plt.plot(np.arange(TRAINING_SIZE_MIN, TRAINING_SIZE_MAX, 20), mse_train_v, 'r--', label=\"Training Error\")\n",
    "plt.plot(np.arange(TRAINING_SIZE_MIN, TRAINING_SIZE_MAX, 20), mse_test_v, 'b-', label=\"Test Error\")\n",
    "plt.xlabel('Dataset Size')\n",
    "plt.ylabel('Mean Squared Error')\n",
    "plt.show()\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9A9VqDTzOdfd"
   },
   "source": [
    "(Add the answer to Task 8 here in text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "djpsaTu_kK3T"
   },
   "source": [
    "## Polynomial Basis Expansion with Ridge and Lasso\n",
    "\n",
    "For this part use the following from the scikit-learn package. Read the documentation available here: http://scikit-learn.org/stable/modules/classes.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pnw2FEvqkdV_"
   },
   "source": [
    "You will need the use the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9TM0nkNbkhfM"
   },
   "outputs": [],
   "source": [
    "# You will need the following libs. \n",
    "# Fell free to import other libs. \n",
    "\n",
    "# import the preprocessing libs for standarization and basis expansion\n",
    "from sklearn import linear_model\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures \n",
    "from sklearn.linear_model import Ridge, Lasso \n",
    "\n",
    "def standardization(X):\n",
    "    scalar = StandardScaler()\n",
    "    X_scaled = scalar.fit_transform(X)\n",
    "    return X_scaled\n",
    "# Ridge and Lasso linear model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9fAfOfXCksT9"
   },
   "source": [
    "Try 5 powers of 10 for lambda from 10^-2 to 10^2 and use degree 2 basis expansion. Fit ridge and lasso using degree 2 polynomial expansion with these values of lambda. You should pick the optimal values for lambda using a validation set. Set the last 20% of the training set for the purpose of validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pCwBPuOXlRF7"
   },
   "source": [
    "### **Task 9**\n",
    "Let's implement the function for expanding the basis of the dataset. \n",
    "\n",
    "Hints: use `PolynomialFeatures`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "50azFolql1qA"
   },
   "outputs": [],
   "source": [
    "def expand_basis(X, degree):\n",
    "    # TODO: expand the basis of X for the degree\n",
    "    # The code below is just for compilation. \n",
    "    # You need to replace it by your own code.\n",
    "    ###################################################\n",
    "    ##### YOUR CODE STARTS HERE #######################\n",
    "    ###################################################\n",
    "    poly = PolynomialFeatures(degree)\n",
    "    X = poly.fit_transform(X)\n",
    "    return X\n",
    "    ###################################################\n",
    "    ##### YOUR CODE ENDS HERE #########################\n",
    "    ###################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6jwkPevimQri"
   },
   "source": [
    "### **Task 10**\n",
    "Prepare the training, test and validation data using the expanded dataset. Expand and standardize the the data. \n",
    "\n",
    "Hints: you can use `StandardScaler` and `std_scaler` to standardize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dQCq4G9YmW7w"
   },
   "outputs": [],
   "source": [
    "# TODO: the training, test and validation data using the expanded dataset.\n",
    "# The code below is just for compilation. \n",
    "# You need to replace it by your own code.\n",
    "def prepare_data(X, y, degree):\n",
    "    ###################################################\n",
    "    ##### YOUR CODE STARTS HERE #######################\n",
    "    ###################################################\n",
    "    # Hints: follow the steps    \n",
    "    # You need to parpare four datasets:\n",
    "    # 1. training data -- X_train, y_train\n",
    "    # 2. test data -- X_test, y_test\n",
    "    # 3. validation data -- X_train_v, y_train_v\n",
    "    # 4. training data (cross validation) -- X_train_n, y_train_n\n",
    "    \n",
    "    # You need expand the basis of the data, and do standardization\n",
    "\n",
    "    # training data\n",
    "    X = expand_basis(X, degree)\n",
    "    X = standardization(X)\n",
    "    X_train, y_train, X_test, y_test = split_data(X, y, 0.8)\n",
    "\n",
    "    X_train_n, y_train_n, X_train_v, y_train_v = split_data(X_train, y_train, 0.8)\n",
    "\n",
    "    return X_train, y_train, X_train_n, y_train_n, X_train_v, y_train_v, X_test, y_test\n",
    "    ###################################################\n",
    "    ##### YOUR CODE ENDS HERE #########################\n",
    "    ###################################################\n",
    "\n",
    "X_train, y_train, X_train_n, y_train_n, X_train_v, y_train_v, X_test, y_test = prepare_data(X, y, 2) # here we expand the dataset with degree 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i3BxxtM3nghU"
   },
   "source": [
    "### **Task 11**\n",
    "We have prepared the training data and the validation data. We can now choose the hyper parameter lambda for Ridge and Lasso using the validation data. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 555
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3266,
     "status": "ok",
     "timestamp": 1596436131187,
     "user": {
      "displayName": "Haozhe Zhang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhewCb1FImnjURCLugyfalL8wVXJomnuoEHUckN=s64",
      "userId": "15943369882491692800"
     },
     "user_tz": -480
    },
    "id": "SvXcAGW1oHq1",
    "outputId": "25a38d1f-013f-4b0a-9cbb-3f08b68c0371"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge lambda: 0.01\n",
      "Lasso lambda: 0.01\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# The function takes the training and validation data as inputs, and \n",
    "# returns the lambda value that has the minimal mse\n",
    "# We use is_ridge to indicate the model we consider. \n",
    "# is_ridge = True indicates Ridge while is_ridge = False indicates Lasso\n",
    "def choose_hyper_param(X_train_n, y_train_n, X_train_v, y_train_v, is_ridge: bool):\n",
    "    mse_arr = []\n",
    "    lam_arr = []\n",
    "\n",
    "    # Try lambda values from 10^-2 to 10^2. \n",
    "    # Record the mse and the lambda values in mse_arr and lam_arr\n",
    "    # The code below is just for compilation. \n",
    "    # You need to replace it by your own code.\n",
    "    ###################################################\n",
    "    ##### YOUR CODE STARTS HERE #######################\n",
    "    ###################################################\n",
    "    for pow_lam in range(-2, 3):\n",
    "        lam = 10 ** pow_lam\n",
    "        if is_ridge:\n",
    "            clf = Ridge(lam)\n",
    "            \n",
    "        else:\n",
    "            clf = Lasso(lam)\n",
    "    \n",
    "        clf.fit(X_train_n, y_train_n)\n",
    "        y_predicted = clf.predict(X_train_v)\n",
    "        errors=(y_train_v-y_predicted)\n",
    "        squared_errors=errors**2\n",
    "        sum_squared_errors =np.sum(squared_errors)\n",
    "        mse = 1/(y_train_v.size)*sum_squared_errors\n",
    "            \n",
    "        mse_arr.append(mse) # add the mse when using the hyperparameter lam\n",
    "        lam_arr.append(lam)\n",
    "    ###################################################\n",
    "    ##### YOUR CODE ENDS HERE #########################\n",
    "    ###################################################\n",
    "\n",
    "\n",
    "    # get the index of the lambda value that has the minimal use\n",
    "    lambda_idx_min = np.argmin(np.array(mse_arr))\n",
    "    # print(lam_arr[lambda_idx_min])\n",
    "\n",
    "    # plot of the lambda values and their mse\n",
    "    plt.figure()\n",
    "    plt.semilogx(lam_arr, mse_arr)\n",
    "\n",
    "    # return the best lambda value\n",
    "    return lam_arr[lambda_idx_min]\n",
    "\n",
    "# call the function to choose the lambda for Ridge and Lasso\n",
    "lam_ridge = choose_hyper_param(X_train_n, y_train_n, X_train_v, y_train_v, True)\n",
    "lam_lasso = choose_hyper_param(X_train_n, y_train_n, X_train_v, y_train_v, False)\n",
    "\n",
    "print(\"Ridge lambda:\", lam_ridge)\n",
    "print(\"Lasso lambda:\", lam_lasso)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FAuX0uU5k9qD"
   },
   "source": [
    "### **Task 12**:\n",
    "Once you’ve obtained the optimal values for lambda for Ridge and Lasso, train these models using these hyperparameters on the full training data. Then report\n",
    "the training and test error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3259,
     "status": "ok",
     "timestamp": 1596436131187,
     "user": {
      "displayName": "Haozhe Zhang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhewCb1FImnjURCLugyfalL8wVXJomnuoEHUckN=s64",
      "userId": "15943369882491692800"
     },
     "user_tz": -480
    },
    "id": "VmwHESkg77zK",
    "outputId": "9bb9c1cf-1649-40e6-9162-2244525d9446"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Ridge Regression with using degree 2 polynomial expansion and lambda = 0.0100\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "MSE (Training) = 0.5017\n",
      "MSE (Testing)  = 0.5148\n",
      "\n",
      "\n",
      "For Lasso with using degree 2 polynomial expansion and lambda = 0.0100\n",
      "---------------------------------------------------------------------\n",
      "\n",
      "MSE (Training) = 0.5490\n",
      "MSE (Testing)  = 0.5421\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/niveditajha/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.6575181351663559, tolerance: 0.304341322103114\n",
      "  positive)\n"
     ]
    }
   ],
   "source": [
    "# TODO: train the Ridge and Lasso models using their best parameters, and\n",
    "#       report their mse\n",
    "###################################################\n",
    "##### YOUR CODE STARTS HERE #######################\n",
    "###################################################\n",
    "# Hints: train these models on the full training data\n",
    "\n",
    "clf_ridge = ridge(lam_ridge, X_train , y_train)\n",
    "clf_lasso = lasso(lam_lasso, X_train , y_train)\n",
    "\n",
    "def mse_calculation(classifier, X, y):\n",
    "    y_predicted = classifier.predict(X)\n",
    "    errors=(y-y_predicted)\n",
    "    squared_errors=errors**2\n",
    "    sum_squared_errors =np.sum(squared_errors)\n",
    "    mse = 1/(y.size)*sum_squared_errors\n",
    "    return mse\n",
    "\n",
    "mse_ridge_train = mse_calculation(clf_ridge, X_train , y_train)\n",
    "mse_ridge_test = mse_calculation(clf_ridge, X_test , y_test)\n",
    "mse_lasso_train = mse_calculation(clf_lasso, X_train , y_train)\n",
    "mse_lasso_test = mse_calculation(clf_lasso, X_test , y_test)\n",
    "###################################################\n",
    "##### YOUR CODE ENDS HERE #########################\n",
    "###################################################\n",
    "\n",
    "# Report the result\n",
    "print('For Ridge Regression with using degree %d polynomial expansion and lambda = %.4f' % (2, lam_ridge))\n",
    "print('--------------------------------------------------------------------------------\\n')\n",
    "print('MSE (Training) = %.4f' % mse_ridge_train)\n",
    "print('MSE (Testing)  = %.4f' % mse_ridge_test)\n",
    "\n",
    "print('\\n\\nFor Lasso with using degree %d polynomial expansion and lambda = %.4f' % (2, lam_lasso))\n",
    "print('---------------------------------------------------------------------\\n')\n",
    "print('MSE (Training) = %.4f' % mse_lasso_train)\n",
    "print('MSE (Testing)  = %.4f' % mse_lasso_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Os9tKKLd8gMU"
   },
   "source": [
    "## Larger Degrees\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sfqRAlv1PBXi"
   },
   "source": [
    "### **Task 13**\n",
    "Try using higher degree basis expansion. You may want to use k-fold cross validation to determine\n",
    "the values of hyperparameters rather than just keeping a validation set. \n",
    "\n",
    "Hints: Use `KFold` to do this automatically. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kpwY7UtQ8l-0"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "k-fold cross-validation requires at least one train/test split by setting n_splits=2 or more, got n_splits=1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-140-f416fd699fd1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mdegree_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mkf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKFold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_splits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"lets see\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mX_train\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, n_splits, shuffle, random_state)\u001b[0m\n\u001b[1;32m    429\u001b[0m     def __init__(self, n_splits=5, shuffle=False,\n\u001b[1;32m    430\u001b[0m                  random_state=None):\n\u001b[0;32m--> 431\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_splits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    432\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_iter_test_indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, n_splits, shuffle, random_state)\u001b[0m\n\u001b[1;32m    282\u001b[0m                 \u001b[0;34m\"k-fold cross-validation requires at least one\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m                 \u001b[0;34m\" train/test split by setting n_splits=2 or more,\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 284\u001b[0;31m                 \" got n_splits={0}.\".format(n_splits))\n\u001b[0m\u001b[1;32m    285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: k-fold cross-validation requires at least one train/test split by setting n_splits=2 or more, got n_splits=1."
     ]
    }
   ],
   "source": [
    "# KFold\n",
    "from sklearn.model_selection import KFold\n",
    "for degree in range(10):\n",
    "    mse_arr =[]\n",
    "    degree_arr = []\n",
    "    \n",
    "    kf = KFold(n_splits=4)\n",
    "    print(\"lets see\")\n",
    "    train_idx , test_idx = kf.split(X_train, y_train)\n",
    "    X_train = X_train[train_idx]\n",
    "    X_test = X[test_idx]\n",
    "    y_train = y[train_idx]\n",
    "    y_test = y[test_idx]\n",
    "    print(\"lets see2\")\n",
    "    X_train, y_train, X_train_n, y_train_n, X_train_v, y_train_v, X_test, y_test = prepare_data(X_train, y_train, degree)\n",
    "    lam_ridge = choose_hyper_param(X_train_n, y_train_n, X_train_v, y_train_v, True)\n",
    "    lam_lasso = choose_hyper_param(X_train_n, y_train_n, X_train_v, y_train_v, False)\n",
    "    clf_ridge = ridge(lam_ridge, X_train , y_train)\n",
    "#     clf_lasso = lasso(lam_lasso, X_train , y_train)\n",
    "    def mse_calculation(classifier, X, y):\n",
    "        y_predicted = classifier.predict(X)\n",
    "        errors=(y-y_predicted)\n",
    "        squared_errors=errors**2\n",
    "        sum_squared_errors =np.sum(squared_errors)\n",
    "        mse = 1/(y.size)*sum_squared_errors\n",
    "        return mse\n",
    "\n",
    "    mse = mse_calculation(clf_ridge, X_test , y_test)\n",
    "    \n",
    "mse_arr.append(mse)\n",
    "degree_arr.append(degree)\n",
    "    \n",
    "degree_idx_min = np.argmin(np.array(mse_arr))\n",
    "min_degree = degree_arr[degree_idx_min]\n",
    "# TODO: Try using higher degree basis expansion. Find the degree that gives the minimal mse. \n",
    "###################################################\n",
    "##### YOUR CODE STARTS HERE #######################\n",
    "###################################################\n",
    "# Hints: use KFold\n",
    "\n",
    "###################################################\n",
    "##### YOUR CODE ENDS HERE #########################\n",
    "###################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyP96ktvsOI4PiuW52tcNLjx",
   "collapsed_sections": [],
   "name": "Practical1_starter.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
